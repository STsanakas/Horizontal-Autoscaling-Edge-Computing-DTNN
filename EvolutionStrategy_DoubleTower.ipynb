{"cells":[{"cell_type":"code","execution_count":21,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-07-05T09:32:30.004965Z","iopub.status.busy":"2022-07-05T09:32:30.004598Z","iopub.status.idle":"2022-07-05T09:32:30.070108Z","shell.execute_reply":"2022-07-05T09:32:30.069104Z","shell.execute_reply.started":"2022-07-05T09:32:30.004934Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import pickle\n","import numpy as np\n","import tensorflow as tf\n","import keras.backend as K\n","import tqdm, math, re, os, random, time, glob\n","from tqdm.keras import TqdmCallback\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import mean_squared_error\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras import optimizers\n","from tensorflow.keras.models import Model, Sequential\n","from tensorflow.keras.layers import Dense, Flatten, Dropout, LSTM, concatenate, GRU\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow import keras\n","from skopt.space import Categorical, Real\n","from skopt.utils import use_named_args\n","from skopt import gp_minimize\n","import matplotlib.pyplot as plt\n","from statsmodels.tsa.holtwinters import ExponentialSmoothing\n","import numpy.ma as ma\n","\n","def remove_dupes(i, o):\n","    for j in range(len(i)):\n","        for k in range(len(i)):\n","            if (i[j]==i[k] and j!=k):\n","                if (o[j]>o[k]):\n","                    i.pop(j)\n","                    o.pop(j)\n","                    return remove_dupes(i, o)\n","                else:\n","                    i.pop(k)\n","                    o.pop(k)\n","                    return remove_dupes(i, o)\n","    return i, o\n","\n","def average_params(params):\n","    params=np.array(params)\t\n","    best=ma.array((params[0], params[1], params[2], params[3])).mean(axis=0)\t\n","    return best\n","\n","best_error=1\n","bayes_inputs = []\n","bayes_results = []\n","bn=0\n","unitsL=[2,200]\n","epochsL=[20,200]\n","learningrateL=[0.001,0.1]\n","dropoutL=[0.0,0.5]\n","float_formatter = \"{:.5f}\".format\n","np.set_printoptions(formatter={'float_kind':float_formatter})\n","\n","def descale(scaled, limits, is_int=False):\n","  descaled = limits[0] + (scaled*(limits[1]-limits[0]))\n","  if isinstance(limits[1], int):\n","    descaled = int(round(descaled))\n","  return descaled\n","\n","def getOptimizer(opt,lr):  \n","  if opt=='RMSProp':\n","    myopt = optimizers.RMSprop(learning_rate=descale(lr,learningrateL))  \n","  elif opt=='Adam':\n","    myopt = optimizers.Adam(learning_rate=descale(lr,learningrateL))\n","  elif opt=='SGD':    \n","    myopt = optimizers.SGD(learning_rate=descale(lr,learningrateL))\n","  elif opt=='Adagrad':\n","    myopt = optimizers.Adagrad(learning_rate=descale(lr,learningrateL))\n","  elif opt=='Adadelta':\n","    myopt = optimizers.Adadelta(learning_rate=descale(lr,learningrateL))\n","  elif opt=='Adamax':\n","    myopt = optimizers.Adamax(learning_rate=descale(lr,learningrateL))\n","  elif opt=='Nadam':\n","    myopt = optimizers.Nadam(learning_rate=descale(lr,learningrateL))\n","  return myopt  \n","\n","dimensions  = [Categorical(['tanh','sigmoid','linear','relu', 'elu'], name='Recurrent_Dense_activation'),\n","               Categorical(['tanh','sigmoid','linear','relu', 'elu'], name='Feedforward_Dense_activation_1'),\n","               Categorical(['tanh','sigmoid','linear','relu', 'elu'], name='Feedforward_Dense_activation_2'),\n","               Categorical(['tanh','sigmoid','linear','relu', 'elu'], name='Feedforward_Dense_activation_3'),\n","               Categorical(['tanh','sigmoid','linear','relu', 'elu'], name='Concatenated_Dense_activation'),\n","               Categorical(['tanh','sigmoid','linear','relu', 'elu'], name='Output_activation'),               \n","               Categorical(['RMSProp','Adam','SGD','Adagrad', 'Adadelta', 'Adamax', 'Nadam'], name='optimizer')]\n","defact = ['tanh','sigmoid','linear','relu', 'elu']\n","defopt =  ['RMSProp','Adam','SGD','Adagrad', 'Adadelta', 'Adamax', 'Nadam']\n","default_parameters = [random.choice(defact), random.choice(defact), random.choice(defact), random.choice(defact), random.choice(defact), random.choice(defact), random.choice(defopt)]\n","@use_named_args(dimensions=dimensions) #Combine Objective function with its search space\n","\n","\n","def compile_model(Recurrent_Dense_activation, Feedforward_Dense_activation_1, Feedforward_Dense_activation_2, Feedforward_Dense_activation_3, Concatenated_Dense_activation, Output_activation, optimizer):  \n","  global best_error\n","  Recurrent = Sequential()\n","  if myparams[0]>0.5:\n","    Recurrent.add(LSTM(descale(myparams[1], unitsL), activation='tanh', dropout=descale(myparams[2], dropoutL), recurrent_activation='sigmoid', input_shape=(trainX[0].shape[1], trainX[0].shape[2]), return_sequences=True))  \n","  Recurrent.add(LSTM(descale(myparams[3], unitsL), activation='tanh', dropout=descale(myparams[4], dropoutL), recurrent_activation='sigmoid', input_shape=(trainX[0].shape[1], trainX[0].shape[2])))  \n","  Recurrent.add(Flatten())\n","  if myparams[5]>0.5:\n","    Recurrent.add(Dense(descale(myparams[6], unitsL), activation=Recurrent_Dense_activation))\n","    Recurrent.add(Dropout(descale(myparams[7], dropoutL)))      \n","  FeedForward = Sequential()\n","  FeedForward.add(Dense(descale(myparams[8], unitsL), input_shape=(trainX[1].shape[1],), activation=Feedforward_Dense_activation_1))\n","  FeedForward.add(Dropout(descale(myparams[9], dropoutL)))\n","  if myparams[10]>0.5:\n","    FeedForward.add(Dense(descale(myparams[11], unitsL), activation=Feedforward_Dense_activation_2))\n","    FeedForward.add(Dropout(descale(myparams[12], dropoutL)))\n","  if myparams[13]>0.5:\n","    FeedForward.add(Dense(descale(myparams[14], unitsL), activation=Feedforward_Dense_activation_3))\n","    FeedForward.add(Dropout(descale(myparams[15], dropoutL)))\n","\n","  conc = concatenate([Recurrent.output, FeedForward.output])\n","  flat = Flatten()(conc)\n","  if myparams[16]>0.5:\n","    dense = Dense(descale(myparams[17], unitsL), activation=Concatenated_Dense_activation)(flat)\n","    dropout = Dropout(descale(myparams[18], dropoutL))(dense)\n","    prediction = Dense(1, activation=Output_activation)(dropout)\n","  else:\n","    prediction = Dense(1, activation=Output_activation)(flat)\n","  model = Model([Recurrent.input, FeedForward.input], prediction) \n","\n","  callback=tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001, restore_best_weights=True)\n","  mymetrics=['mean_squared_error']\n","\n","  model.compile(loss='mean_absolute_error', metrics=mymetrics, optimizer=getOptimizer(optimizer,descale(myparams[19], learningrateL)))\n","  history = model.fit(trainX, trainY, epochs=100, batch_size=320, verbose=0, validation_split=0.2, shuffle=False, callbacks=[callback])\n","  loss, mse = model.evaluate(testX, testY, verbose=0)\n","  if (loss<best_error):\n","    files = glob.glob(home_folder+'models/*')\n","    for f in files:\n","      os.remove(f)\n","    best_error = loss\n","    model.save(home_folder+'models/Model_'+str(loss)+'.h5')\n","  return loss\n","\n","\n","def eval_model(params):\n","\tglobal best_error, dataset, myparams, bn, bayes_inputs, bayes_results, bayes_time, bt\n","\tcalls=3\n","\tmyparams=params\t\n","\tif not bayes_inputs:\n","\t\tbayes = gp_minimize(func=compile_model, dimensions=dimensions, acq_func='EI', n_calls=calls, n_random_starts=1, x0=default_parameters, model_queue_size=1)\n","\telse:\n","\t\tbayes = gp_minimize(func=compile_model, dimensions=dimensions, acq_func='EI', n_calls=calls, n_random_starts=1, x0=bayes_inputs, y0=bayes_results, model_queue_size=1)\n","\tbn=bn+1\t\n","\tbayes_inputs.extend(bayes.x_iters)\n","\tbayes_results.extend(bayes.func_vals) \t\t\n","\tkeep=calls*bn\n","\tbayes_inputs=bayes_inputs[-keep:]\n","\tbayes_results=bayes_results[-keep:]\n","\tbayes_inputs, bayes_results=remove_dupes(bayes_inputs, bayes_results)\n","\tpars=bayes_inputs[bayes_results.index(min(bayes_results[-calls:]))]\n","\terror=min(bayes_results[-calls:])\n","\tprint(round(bayes_results[bayes_results.index(min(bayes_results[-calls:]))],4))\n","\treturn error\n","\n","def Evolution_Strategy(top_n = 5, n_pop = 20, n_iter = 10, sigma_error = 0.15, error_weight = 1, decay_rate = 0.95, min_error_weight = 0.01 ):\n"," # Model weights have been randomly initialized at first\n"," params = [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]\n","\n"," best_params = params\n"," model_evaluation_values=\"\" \n"," i=0\n"," if os.path.exists(home_folder+'state.pkl'):\n","    print('Found a previously saved state')\n","    with open(home_folder+'state.pkl', 'rb') as handle:\n","        state = pickle.load(handle)\n","    model_evaluation_values=state['values']\n","    i=state['iteration']\n","    best_params=Ïƒ['best_params']\n","    print('resuming from iteration', i)\n"," while i <n_iter:\n"," # Generating the population of parameters\n","\t print(time.time() - timezero, 'gen ', i)\n","\t pop_params = [best_params + error_weight*sigma_error*np.random.randn(*np.shape(params)) for i in range(n_pop)]\n","\t pop_params = [x.clip(0, 1)  for x in pop_params] \n"," # Evaluating the population of parameters\n","\t evaluation_values=[eval_model(parameters) for parameters in pop_params]\n","\t average=np.average(evaluation_values)\n","\t for element in evaluation_values:\n","\t\t model_evaluation_values=model_evaluation_values+str(element)+\",\"\n","\t model_evaluation_values=model_evaluation_values+(\"\\b\\n\")\n"," # Sorting based on evaluation score\n","\t param_eval_list = zip(evaluation_values, pop_params) \n","\t param_eval_list = sorted(param_eval_list, key = lambda x: x[0], reverse = False)\n","\t evaluation_values, pop_params = zip(*param_eval_list) \n"," # Taking the mean of the elite parameters \n","\t best_params = average_params(pop_params[:top_n])\n"," # Decaying the weight\n","\t error_weight = max(error_weight*decay_rate, min_error_weight) \n","\t params = best_params\n","\t i+=1\n","\t state={}\n","\t state['values']=model_evaluation_values\n","\t state['iteration']=i\n","\t state['best_params']=best_params\n","\t state_f = open(home_folder+'state.pkl',\"wb\")\n","\t pickle.dump(state,state_f)\n","\t state_f.close()\n","\t f = open(home_folder+\"EvolutionStrategyConvergence.csv\", \"w\")\n","\t f.write(model_evaluation_values)\n","\t f.close()\n"," print(model_evaluation_values)\n"," os.remove(home_folder+'state.pkl')\n"," return best_params, param_eval_list[0][0], average # Instantiating our model object"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2022-07-05T09:32:30.998994Z","iopub.status.busy":"2022-07-05T09:32:30.998358Z","iopub.status.idle":"2022-07-05T09:36:02.860170Z","shell.execute_reply":"2022-07-05T09:36:02.858052Z","shell.execute_reply.started":"2022-07-05T09:32:30.998957Z"},"trusted":true},"outputs":[],"source":["#expects a trainX list which consists of the numpy RNN input (samples, timesteps, features) and the numpy Feedforward input (samples, features)\n","#same goes for testX\n","#trainY and testY should contain a single output (samples, output)\n","#home_folder is used to save the best model, the models' results and the state at the start of each iteration as a checkpoint so it can resume if needed\n","home_folder='./results/'\n","best_params, best_periteration, average_periteration= Evolution_Strategy(top_n = 4, n_pop = 10, n_iter = 10)\n","print(time.time() - timezero, 'algorithm finished')"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
